{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import getenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the base path\n",
    "base_path = Path(\"../\")  # One level up from the current working directory\n",
    "\n",
    "# Add the src/ directory to sys.path using base_path\n",
    "sys.path.append(str((base_path / \"src\").resolve()))\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from utils.yaml_functions import load_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = load_yaml(base_path / \"decision_problems/oil_field_purchase/problem.yaml\")[\"long_version\"]\n",
    "\n",
    "objective = \"\"\"\n",
    "## Objective\n",
    "\n",
    "You are acting as a decision analysis expert. Based on the problem description above, your task is to:\n",
    "\n",
    "1. Analyze the decision problem using principles of decision theory.\n",
    "2. Compute the expected utility of all relevant options, considering both the possibility of testing and not testing.\n",
    "3. Determine whether the company should perform the geological test.\n",
    "4. Based on the test result (if the test is chosen), decide whether the company should purchase the oil field.\n",
    "5. Recommend the optimal strategy that maximizes expected value for the company.\n",
    "\n",
    "Make sure your reasoning is explicit, and show all calculations, including conditional probabilities, expected utilities, and decision comparisons. Your final answer should include:\n",
    "- The recommended overall strategy (test or not, and what to do based on the outcome)\n",
    "- Justification with relevant numeric reasoning\n",
    "\"\"\"\n",
    "\n",
    "prompt_str = \"{problem}\" + \"\\n\" + objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Oil Field Investment Decision Problem  \n",
      "\n",
      "### Problem Overview  \n",
      "An oil company must decide whether to **buy (B)** an oil field whose true quality **Q** is uncertain.  \n",
      "\n",
      "Q has three possible states:\n",
      "- **High**: Exceptional reserves, high-profit potential  \n",
      "- **Medium**: Moderate reserves, acceptable profit  \n",
      "- **Low**: Poor reserves, high financial risk  \n",
      "\n",
      "Historical and survey data give the prior probabilities:\n",
      "- P(Q = High) = 0.30  \n",
      "- P(Q = Medium) = 0.40  \n",
      "- P(Q = Low)   = 0.30  \n",
      "\n",
      "### Information-Gathering Option  \n",
      "The company may optionally perform a **Porosity Test** at cost \\$0.01 B (i.e.\\ \\$10 million):  \n",
      "- **Result R = “Pass”** if porosity ≥ 15%  \n",
      "- **Result R = “Fail”** if porosity < 15%  \n",
      "- Or skip testing and go straight to the purchase decision.\n",
      "\n",
      "#### Test Reliability \n",
      "\n",
      "The test provides imperfect but valuable information about the true quality of the field. We can represent the reliability of the test as a conditional probability distribution P(R | Q) where:\n",
      "\n",
      "**High Quality Fields:**  \n",
      "- P(Pass|high) = 0.90 → Strong indicator of quality  \n",
      "- P(Fail|high) = 0.10 → Rare false negative  \n",
      "\n",
      "**Medium Quality Fields:**  \n",
      "- P(Pass|medium) = 0.65 → Moderate correlation  \n",
      "- P(Fail|medium) = 0.35  \n",
      "\n",
      "**Low Quality Fields:**  \n",
      "- P(Pass|low) = 0.15 → Rare false positive  \n",
      "- P(Fail|low) = 0.85 → Strong indicator of poor quality  \n",
      "\n",
      "### Outcome Utilities  \n",
      "To evaluate the decision problem, we need to define the value (utility) of each outcome. This utility reflects the desirability of a particular scenario (buying the field after a successful test, etc.).\n",
      "\n",
      "#### If Test Performed  \n",
      "- **Buy (after observing R):**  \n",
      "  - Q = High → +\\$0.85 B  \n",
      "  - Q = Medium → +\\$0.43 B  \n",
      "  - Q = Low → +\\$0.00 B  \n",
      "- **Do Not Buy:** +\\$0.25 B  \n",
      "\n",
      "*(Note: test cost \\$0.01 B is already subtracted from all of these figures.)*\n",
      "\n",
      "#### If No Test  \n",
      "- **Buy (direct):**  \n",
      "  - Q = High → +\\$0.86 B  \n",
      "  - Q = Medium → +\\$0.44 B  \n",
      "  - Q = Low → +\\$0.01 B  \n",
      "- **Do Not Buy:** +\\$0.26 B  \n",
      "\n",
      "### Decision Sequence  \n",
      "1. Decide whether to perform the Porosity Test.  \n",
      "2. If tested, observe result R.  \n",
      "3. Decide whether to buy the oil field.  \n",
      "\n",
      "## Objective\n",
      "\n",
      "You are acting as a decision analysis expert. Based on the problem description above, your task is to:\n",
      "\n",
      "1. Analyze the decision problem using principles of decision theory.\n",
      "2. Compute the expected utility of all relevant options, considering both the possibility of testing and not testing.\n",
      "3. Determine whether the company should perform the geological test.\n",
      "4. Based on the test result (if the test is chosen), decide whether the company should purchase the oil field.\n",
      "5. Recommend the optimal strategy that maximizes expected value for the company.\n",
      "\n",
      "Make sure your reasoning is explicit, and show all calculations, including conditional probabilities, expected utilities, and decision comparisons. Your final answer should include:\n",
      "- The recommended overall strategy (test or not, and what to do based on the outcome)\n",
      "- Justification with relevant numeric reasoning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openrouter = \"gpt-4o-mini\"\n",
    "request_timeout = 10\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(template=prompt_str, input_variables=[\"text\"])\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    model_name=model_openrouter,\n",
    "    request_timeout=request_timeout,\n",
    ")\n",
    "\n",
    "# Initialize the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain for processing the text\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I’m ChatGPT, your AI assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from os import getenv\n",
    "\n",
    "model_openrouter = \"gpt-4o-mini\"\n",
    "request_timeout = 10\n",
    "\n",
    "prompt_str = \"Say hello and tell me your name.\"\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(template=prompt_str, input_variables=[])\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    model_name=model_openrouter,\n",
    "    request_timeout=request_timeout,\n",
    ")\n",
    "\n",
    "# Initialize the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain for processing the text\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Run the chain and print the output\n",
    "response = chain.invoke({})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "{'message': '\"functions\" and \"function_call\" are deprecated in favor of \"tools\" and \"tool_choice.\" To learn how to use tools, visit: https://openrouter.ai/docs/requests#tool-calls', 'code': 400, 'metadata': {'provider_name': 'Azure'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     30\u001b[39m agent = initialize_agent(\n\u001b[32m     31\u001b[39m     tools=tools,\n\u001b[32m     32\u001b[39m     llm=llm,\n\u001b[32m     33\u001b[39m     agent=AgentType.OPENAI_FUNCTIONS,  \u001b[38;5;66;03m# <-- Use OPENAI_TOOLS instead of OPENAI_FUNCTIONS\u001b[39;00m\n\u001b[32m     34\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m query = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[33mGiven P(A) = 0.3, P(B|A) = 0.8, and P(B) = 0.5, calculate P(A|B) using Bayes theorem.\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[33mThen calculate P(not A | B).\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain/chains/base.py:603\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m1\u001b[39m:\n\u001b[32m    602\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1620\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1628\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1629\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1630\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1326\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1319\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1324\u001b[39m ) -> Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1336\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1354\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1351\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1353\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_action_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1360\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain/agents/openai_functions_agent/base.py:124\u001b[39m, in \u001b[36mOpenAIFunctionsAgent.plan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, with_functions, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m messages = prompt.to_messages()\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_functions:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     predicted_message = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    130\u001b[39m     predicted_message = \u001b[38;5;28mself\u001b[39m.llm.predict_messages(\n\u001b[32m    131\u001b[39m         messages,\n\u001b[32m    132\u001b[39m         callbacks=callbacks,\n\u001b[32m    133\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1267\u001b[39m, in \u001b[36mBaseChatModel.predict_messages\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m0.1.7\u001b[39m\u001b[33m\"\u001b[39m, alternative=\u001b[33m\"\u001b[39m\u001b[33minvoke\u001b[39m\u001b[33m\"\u001b[39m, removal=\u001b[33m\"\u001b[39m\u001b[33m1.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1258\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_messages\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1264\u001b[39m     **kwargs: Any,\n\u001b[32m   1265\u001b[39m ) -> BaseMessage:\n\u001b[32m   1266\u001b[39m     _stop = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1191\u001b[39m, in \u001b[36mBaseChatModel.__call__\u001b[39m\u001b[34m(self, messages, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m0.1.7\u001b[39m\u001b[33m\"\u001b[39m, alternative=\u001b[33m\"\u001b[39m\u001b[33minvoke\u001b[39m\u001b[33m\"\u001b[39m, removal=\u001b[33m\"\u001b[39m\u001b[33m1.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1171\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1175\u001b[39m     **kwargs: Any,\n\u001b[32m   1176\u001b[39m ) -> BaseMessage:\n\u001b[32m   1177\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model.\u001b[39;00m\n\u001b[32m   1178\u001b[39m \n\u001b[32m   1179\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m \u001b[33;03m        The model output message.\u001b[39;00m\n\u001b[32m   1190\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m     generation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m   1194\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[32m   1195\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation.message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:766\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    765\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m         )\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1012\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1016\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:960\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    959\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.client.create(**payload)\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/decision-theory-llms/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1001\u001b[39m, in \u001b[36mBaseChatOpenAI._create_chat_result\u001b[39m\u001b[34m(self, response, generation_info)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;66;03m# Sometimes the AI Model calling will get error, we should raise it.\u001b[39;00m\n\u001b[32m    997\u001b[39m \u001b[38;5;66;03m# Otherwise, the next code 'choices.extend(response[\"choices\"])'\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[38;5;66;03m# will throw a \"TypeError: 'NoneType' object is not iterable\" error\u001b[39;00m\n\u001b[32m    999\u001b[39m \u001b[38;5;66;03m# to mask the true error. Because 'response[\"choices\"]' is None.\u001b[39;00m\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_dict.get(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(response_dict.get(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1003\u001b[39m token_usage = response_dict.get(\u001b[33m\"\u001b[39m\u001b[33musage\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m response_dict[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mValueError\u001b[39m: {'message': '\"functions\" and \"function_call\" are deprecated in favor of \"tools\" and \"tool_choice.\" To learn how to use tools, visit: https://openrouter.ai/docs/requests#tool-calls', 'code': 400, 'metadata': {'provider_name': 'Azure'}}"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numexpr\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate expression using Python's numexpr library.\n",
    "\n",
    "    Expression should be a single line mathematical expression\n",
    "    that solves the problem.\n",
    "\n",
    "    Examples:\n",
    "        \"37593 * 67\" for \"37593 times 67\"\n",
    "        \"37593**(1/5)\" for \"37593^(1/5)\"\n",
    "    \"\"\"\n",
    "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "    return str(\n",
    "        numexpr.evaluate(\n",
    "            expression.strip(),\n",
    "            global_dict={},  # restrict access to globals\n",
    "            local_dict=local_dict,  # add common mathematical functions\n",
    "        )\n",
    "    )\n",
    "\n",
    "tools = [calculator]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,  # <-- Use OPENAI_TOOLS instead of OPENAI_FUNCTIONS\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "Given P(A) = 0.3, P(B|A) = 0.8, and P(B) = 0.5, calculate P(A|B) using Bayes theorem.\n",
    "Then calculate P(not A | B).\n",
    "\"\"\"\n",
    "\n",
    "print(agent.run(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "from openai import OpenAI\n",
    "\n",
    "# You can use any model that supports tool calling\n",
    "# MODEL = \"google/gemini-2.0-flash-001\"\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "openai_client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "task = \"What are the titles of some James Joyce books?\"\n",
    "\n",
    "messages = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": task,\n",
    "  }\n",
    "]\n",
    "\n",
    "def search_gutenberg_books(search_terms):\n",
    "    search_query = \" \".join(search_terms)\n",
    "    url = \"https://gutendex.com/books\"\n",
    "    response = requests.get(url, params={\"search\": search_query})\n",
    "\n",
    "    simplified_results = []\n",
    "    for book in response.json().get(\"results\", []):\n",
    "        simplified_results.append({\n",
    "            \"id\": book.get(\"id\"),\n",
    "            \"title\": book.get(\"title\"),\n",
    "            \"authors\": book.get(\"authors\")\n",
    "        })\n",
    "\n",
    "    return simplified_results\n",
    "\n",
    "tools = [\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"search_gutenberg_books\",\n",
    "      \"description\": \"Search for books in the Project Gutenberg library based on specified search terms\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"search_terms\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            \"description\": \"List of search terms to find books in the Gutenberg library (e.g. ['dickens', 'great'] to search for books by Dickens with 'great' in the title)\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"search_terms\"]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "TOOL_MAPPING = {\n",
    "    \"search_gutenberg_books\": search_gutenberg_books\n",
    "}\n",
    "\n",
    "request_1 = {\n",
    "    \"model\": \"google/gemini-2.0-flash-001\",\n",
    "    \"tools\": tools,\n",
    "    \"messages\": messages\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='tool_0_search_gutenberg_books', function=Function(arguments='{\"search_terms\":[\"james\",\"joyce\"]}', name='search_gutenberg_books'), type='function', index=0)], reasoning=None)\n"
     ]
    }
   ],
   "source": [
    "response_1 = openai_client.chat.completions.create(**request_1)\n",
    "message = response_1.choices[0].message\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='tool_0_search_gutenberg_books', function=Function(arguments='{\"search_terms\":[\"james\",\"joyce\"]}', name='search_gutenberg_books'), type='function', index=0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the response to the messages array so the LLM has the full context\n",
    "# It's easy to forget this step!\n",
    "messages.append(message)\n",
    "\n",
    "# Now we process the requested tool calls, and use our book lookup tool\n",
    "for tool_call in message.tool_calls:\n",
    "    '''\n",
    "    In this case we only provided one tool, so we know what function to call.\n",
    "    When providing multiple tools, you can inspect `tool_call.function.name`\n",
    "    to figure out what function you need to call locally.\n",
    "    '''\n",
    "    tool_name = tool_call.function.name\n",
    "    tool_args = json.loads(tool_call.function.arguments)\n",
    "    tool_response = TOOL_MAPPING[tool_name](**tool_args)\n",
    "    messages.append({\n",
    "      \"role\": \"tool\",\n",
    "      \"tool_call_id\": tool_call.id,\n",
    "      \"name\": tool_name,\n",
    "      \"content\": json.dumps(tool_response),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some titles of books by James Joyce:\n",
      "\n",
      "1. **Ulysses**\n",
      "2. **Dubliners**\n",
      "3. **A Portrait of the Artist as a Young Man**\n",
      "4. **Chamber Music**\n",
      "5. **Exiles: A Play in Three Acts**\n",
      "6. **Index of the Project Gutenberg Works of James Joyce**\n",
      "\n",
      "These are notable works by Joyce, reflecting his contributions to literature.\n"
     ]
    }
   ],
   "source": [
    "request_2 = {\n",
    "  \"model\": MODEL,\n",
    "  \"messages\": messages,\n",
    "  \"tools\": tools\n",
    "}\n",
    "\n",
    "response_2 = openai_client.chat.completions.create(**request_2)\n",
    "\n",
    "print(response_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def run_tool_call_loop(openai_client, model, tools, messages, tool_mapping, max_iterations=5):\n",
    "    \"\"\"\n",
    "    Repeatedly calls the LLM, handles any tool requests, and returns final response.\n",
    "\n",
    "    :param openai_client: Initialized OpenAI client for OpenRouter.\n",
    "    :param model: The model to use (e.g., 'openai/gpt-4', 'google/gemini-2.0-pro').\n",
    "    :param tools: List of available tool specifications (OpenAI-style function definitions).\n",
    "    :param messages: Initial message list (system + user prompts).\n",
    "    :param tool_mapping: Dictionary mapping tool names to Python functions.\n",
    "    :param max_iterations: Safety cap to avoid infinite loops.\n",
    "    :return: Final message content from the LLM.\n",
    "    \"\"\"\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            tools=tools,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        msg = response.choices[0].message\n",
    "        messages.append(msg.model_dump())\n",
    "\n",
    "        # If no tool calls, print and exit\n",
    "        if not getattr(msg, \"tool_calls\", None):\n",
    "            print(\"🔚 Final model output:\\n\", msg.content[:500])\n",
    "            return msg.content, messages\n",
    "\n",
    "        # Tool call(s) detected\n",
    "        for tool_call in msg.tool_calls:\n",
    "            tool_name = tool_call.function.name\n",
    "            tool_args = json.loads(tool_call.function.arguments)\n",
    "            tool_func = tool_mapping.get(tool_name)\n",
    "\n",
    "            if not tool_func:\n",
    "                raise ValueError(f\"No tool function mapped for '{tool_name}'\")\n",
    "\n",
    "            print(f\"🛠️  Model requested tool: {tool_name} with args: {tool_args}\")\n",
    "            tool_result = tool_func(**tool_args)\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"name\": tool_name,\n",
    "                \"content\": json.dumps(tool_result),\n",
    "            })\n",
    "\n",
    "    print(\"⚠️ Max iterations reached without final response.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "# Messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the titles of some Charles Dickens books?\"}\n",
    "]\n",
    "\n",
    "# Tools + Mapping\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_gutenberg_books\",\n",
    "            \"description\": \"Search for books in the Project Gutenberg library.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_terms\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"Search terms for book lookup\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_terms\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def search_gutenberg_books(search_terms):\n",
    "    import requests\n",
    "    resp = requests.get(\"https://gutendex.com/books\", params={\"search\": \" \".join(search_terms)})\n",
    "    return [{\"title\": b[\"title\"]} for b in resp.json().get(\"results\", [])[:5]]\n",
    "\n",
    "TOOL_MAPPING = {\n",
    "    \"search_gutenberg_books\": search_gutenberg_books\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteration 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/gybx85kd6f147h3rp8wgdz9w0000gn/T/ipykernel_9660/1360868788.py:25: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  messages.append(msg.dict())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️  Model requested tool: search_gutenberg_books with args: {'search_terms': ['Charles Dickens']}\n",
      "\n",
      "--- Iteration 2 ---\n",
      "🔚 Final model output:\n",
      " Here are some titles of books by Charles Dickens:\n",
      "\n",
      "1. A Tale of Two Cities\n",
      "2. Great Expectations\n",
      "3. A Christmas Carol in Prose; Being a Ghost Story of Christmas\n",
      "4. Oliver Twist\n",
      "5. Hard Times\n"
     ]
    }
   ],
   "source": [
    "# Run it\n",
    "result = run_tool_call_loop(\n",
    "    openai_client=client,\n",
    "    model=\"gpt-4o-mini\",  # or any tool-calling-capable model\n",
    "    tools=tools,\n",
    "    messages=messages,\n",
    "    tool_mapping=TOOL_MAPPING\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Here are some titles of books by Charles Dickens:\\n\\n1. A Tale of Two Cities\\n2. Great Expectations\\n3. A Christmas Carol in Prose; Being a Ghost Story of Christmas\\n4. Oliver Twist\\n5. Hard Times',\n",
       " 'refusal': None,\n",
       " 'role': 'assistant',\n",
       " 'annotations': None,\n",
       " 'audio': None,\n",
       " 'function_call': None,\n",
       " 'tool_calls': None,\n",
       " 'reasoning': None}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
